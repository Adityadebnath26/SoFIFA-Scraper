{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d156bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape player hyperlinks and names from Sofifa.com using offset pagination...\n",
      "\n",
      "--- Scraping Page (Offset: 0, Attempting URL: https://sofifa.com/players?offset=0) ---\n",
      "Collected 60 player entries from page (Offset: 0).\n",
      "Added 60 new unique entries from page (Offset: 0).\n",
      "\n",
      "--- Scraping Page (Offset: 60, Attempting URL: https://sofifa.com/players?offset=60) ---\n",
      "Collected 60 player entries from page (Offset: 60).\n",
      "Added 60 new unique entries from page (Offset: 60).\n",
      "\n",
      "--- Scraping Page (Offset: 120, Attempting URL: https://sofifa.com/players?offset=120) ---\n",
      "Collected 60 player entries from page (Offset: 120).\n",
      "Added 55 new unique entries from page (Offset: 120).\n",
      "\n",
      "--- Scraping Page (Offset: 180, Attempting URL: https://sofifa.com/players?offset=180) ---\n",
      "Collected 60 player entries from page (Offset: 180).\n",
      "Added 56 new unique entries from page (Offset: 180).\n",
      "\n",
      "--- Scraping Page (Offset: 240, Attempting URL: https://sofifa.com/players?offset=240) ---\n",
      "Collected 60 player entries from final page (Offset: 240).\n",
      "Added 56 new unique entries from page (Offset: 240).\n",
      "\n",
      "Scraping complete!\n",
      "All collected unique player data saved to 'sofifa_player_data_offset.csv'\n",
      "Total unique player entries extracted: 287\n",
      "No pages failed to scrape after retries. No 'failed_sofifa_pages.csv' created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time # Used for pauses between retries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_RETRIES = 3 # Maximum number of times to retry scraping a page if entries are low\n",
    "EXPECTED_ENTRIES_PER_PAGE = 60 # Expected number of player entries on a full page\n",
    "NUM_PAGES_TO_SCRAPE = 5 # Total number of pages to attempt to scrape (offset iterations)\n",
    "\n",
    "# --- WebDriver Setup ---\n",
    "driver = webdriver.Chrome() # Assumes chromedriver is accessible via PATH\n",
    "\n",
    "# List to store lists of [full_hyperlink, player_name] for all collected unique players\n",
    "all_player_data = []\n",
    "# Set to keep track of unique full hyperlinks to prevent duplicates\n",
    "seen_hyperlinks = set()\n",
    "# List to store URLs of pages that failed to yield enough entries after retries\n",
    "failed_urls = []\n",
    "\n",
    "# Base URL for the players list (without offset)\n",
    "base_players_url = \"https://sofifa.com/players\"\n",
    "# Base URL prefix for constructing full player profile links (e.g., https://sofifa.com)\n",
    "base_link_prefix = \"https://sofifa.com\"\n",
    "\n",
    "print(\"Starting to scrape player hyperlinks and names from Sofifa.com using offset pagination...\")\n",
    "\n",
    "# --- Function to extract player name from URL slug ---\n",
    "def extract_player_name_from_href(href_path):\n",
    "    \"\"\"\n",
    "    Extracts the player's name from a URL path like\n",
    "    '/player/259516/joao-lucas-de-souza-cardoso/250040/'\n",
    "    and formats it to 'Joao Lucas De Souza Cardoso'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split the path by '/'\n",
    "        parts = href_path.split('/')\n",
    "        \n",
    "        # The player name slug is typically the 4th part (index 3)\n",
    "        # after '', 'player', 'ID'\n",
    "        # Example: ['','player','259516','joao-lucas-de-souza-cardoso','250040','']\n",
    "        if '-' in parts[5]:\n",
    "            name = parts[5].replace('-', ' ')\n",
    "            name = name.title()\n",
    "            return name\n",
    "        else:\n",
    "            return (parts[5].title())\n",
    "    except IndexError:\n",
    "        # Handle cases where the path might not conform to the expected structure\n",
    "        pass\n",
    "    return \"N/A\" # Return \"N/A\" if name cannot be extracted\n",
    "\n",
    "# --- Main Scraping Loop with Retries ---\n",
    "try:\n",
    "    # Loop for the specified number of pages\n",
    "    for page_index in range(NUM_PAGES_TO_SCRAPE):\n",
    "        current_offset = page_index * EXPECTED_ENTRIES_PER_PAGE # Calculate the offset\n",
    "        current_page_url = f\"{base_players_url}?offset={current_offset}\"\n",
    "        \n",
    "        print(f\"\\n--- Scraping Page (Offset: {current_offset}, Attempting URL: {current_page_url}) ---\")\n",
    "        \n",
    "        retries = 0\n",
    "        page_scraped_successfully = False\n",
    "\n",
    "        while retries < MAX_RETRIES and not page_scraped_successfully:\n",
    "            if retries > 0:\n",
    "                print(f\"Retrying page (Offset: {current_offset}). Attempt {retries + 1}/{MAX_RETRIES}...\")\n",
    "                time.sleep(3) # Wait a bit before retrying\n",
    "\n",
    "            try:\n",
    "                # Navigate to the constructed URL for the current page\n",
    "                driver.get(current_page_url)\n",
    "                \n",
    "                # Wait for the main table body to be present on the page.\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"tbody\"))\n",
    "                )\n",
    "\n",
    "                # Find all <a> tags that are descendants of a <tbody> element\n",
    "                player_elements = driver.find_elements(By.CSS_SELECTOR, \"tbody a[data-tippy-content]\")\n",
    "\n",
    "                current_page_unique_players_added = 0\n",
    "                temp_page_data = [] # Temporarily store data for this attempt\n",
    "                \n",
    "                if not player_elements:\n",
    "                    print(f\"No player links found on page (Offset: {current_offset}) within tables on attempt {retries + 1}.\")\n",
    "                else:\n",
    "                    for player_element in player_elements:\n",
    "                        href_path = player_element.get_attribute(\"href\")\n",
    "                        if href_path:\n",
    "                            full_hyperlink = base_link_prefix + href_path if href_path.startswith('/') else href_path\n",
    "                            player_name = extract_player_name_from_href(href_path)\n",
    "                            \n",
    "                            player_entry = [full_hyperlink, player_name]\n",
    "                            temp_page_data.append(player_entry)\n",
    "                \n",
    "                # Check if enough entries were collected from this attempt\n",
    "                # The last page might legitimately have fewer entries, so we don't retry if it's the last page\n",
    "                is_last_expected_page = (page_index == NUM_PAGES_TO_SCRAPE - 1)\n",
    "                \n",
    "                if len(temp_page_data) >= EXPECTED_ENTRIES_PER_PAGE or is_last_expected_page:\n",
    "                    if not is_last_expected_page: # Don't print for the very last page if it's short\n",
    "                         print(f\"Collected {len(temp_page_data)} player entries from page (Offset: {current_offset}).\")\n",
    "                    else:\n",
    "                         print(f\"Collected {len(temp_page_data)} player entries from final page (Offset: {current_offset}).\")\n",
    "\n",
    "                    # Add unique entries from this page to the main list and seen set\n",
    "                    for entry in temp_page_data:\n",
    "                        if entry[0] not in seen_hyperlinks: # Check uniqueness by hyperlink\n",
    "                            all_player_data.append(entry)\n",
    "                            seen_hyperlinks.add(entry[0])\n",
    "                            current_page_unique_players_added += 1\n",
    "                    \n",
    "                    print(f\"Added {current_page_unique_players_added} new unique entries from page (Offset: {current_offset}).\")\n",
    "                    page_scraped_successfully = True # Mark as successful, break retry loop\n",
    "                else:\n",
    "                    print(f\"Collected {len(temp_page_data)} entries, which is less than expected ({EXPECTED_ENTRIES_PER_PAGE}) on page (Offset: {current_offset}).\")\n",
    "                    retries += 1 # Increment retry counter\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout while waiting for elements on page (Offset: {current_offset}) on attempt {retries + 1}.\")\n",
    "                retries += 1 # Increment retry counter\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Required element (tbody) not found on page (Offset: {current_offset}) on attempt {retries + 1}. Page structure might have changed or page is empty.\")\n",
    "                retries += 1\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred on page (Offset: {current_offset}) on attempt {retries + 1}: {e}.\")\n",
    "                retries += 1\n",
    "        \n",
    "        # If after all retries, the page still wasn't scraped successfully, record its URL\n",
    "        if not page_scraped_successfully:\n",
    "            print(f\"Failed to scrape page (Offset: {current_offset}) after {MAX_RETRIES} retries. Recording URL for future use.\")\n",
    "            failed_urls.append(current_page_url)\n",
    "\n",
    "\n",
    "# --- Finalizing: Close Browser and Save to CSV ---\n",
    "finally:\n",
    "    # Close the browser session, regardless of whether errors occurred\n",
    "    driver.quit()\n",
    "\n",
    "    # Define the CSV file name for successful scrapes\n",
    "    csv_filename = \"sofifa_player_data_offset.csv\"\n",
    "    \n",
    "    # Save all the collected unique player data to a CSV file\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header row for both columns\n",
    "        writer.writerow([\"Player Hyperlink\", \"Player Name\"]) \n",
    "        # Write each player's data (hyperlink and name) to a new row\n",
    "        writer.writerows(all_player_data)\n",
    "\n",
    "    print(f\"\\nScraping complete!\")\n",
    "    print(f\"All collected unique player data saved to '{csv_filename}'\")\n",
    "    print(f\"Total unique player entries extracted: {len(all_player_data)}\")\n",
    "\n",
    "    # --- Save Failed URLs to a separate CSV ---\n",
    "    if failed_urls:\n",
    "        failed_csv_filename = \"failed_sofifa_pages.csv\"\n",
    "        with open(failed_csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Failed Page URL\"]) # Header for failed URLs\n",
    "            for url in failed_urls:\n",
    "                writer.writerow([url])\n",
    "        print(f\"URLs of {len(failed_urls)} failed pages saved to '{failed_csv_filename}' for future review.\")\n",
    "    else:\n",
    "        print(\"No pages failed to scrape after retries. No 'failed_sofifa_pages.csv' created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
